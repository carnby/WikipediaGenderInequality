{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Analysis of Wikipedia Abstracts\n",
    "\n",
    "In this notebook we preprocess the biography overviews from the English DBpedia. We train a model to detect bi-grams in text, and we generate a vocabulary where we count in how many biographies each uni-/bi-gram appears.\n",
    "\n",
    "By [Eduardo Graells-Garrido](http://carnby.github.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "from dbpedia_utils import iter_entities_from\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gensim\n",
    "import json\n",
    "import gzip\n",
    "import dbpedia_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_folder = dbpedia_config.DATA_FOLDER\n",
    "target_folder = dbpedia_config.TARGET_FOLDER\n",
    "abstracts_file = '{0}/long_abstracts_{1}.nt.bz2'.format(source_folder, dbpedia_config.MAIN_LANGUAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load person data to process only biographies present in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "person_data = pd.read_csv('{0}/person_data_en.csv.gz'.format(target_folder), encoding='utf-8', index_col='uri')\n",
    "person_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here we read the biography overviews to train our gensim co-llocations model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentences():\n",
    "    for i, entity in enumerate(iter_entities_from(abstracts_file)):\n",
    "        resource = entity['resource']\n",
    "        if resource in person_data.index:\n",
    "            try:\n",
    "                abstract = entity['abstract'].pop()\n",
    "                if abstract:\n",
    "                    yield list(gensim.utils.tokenize(abstract, deacc=True, lowercase=True))\n",
    "            except KeyError:\n",
    "                continue\n",
    "        \n",
    "bigrams = gensim.models.Phrases(sentences())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigrams.save('{0}/biography_overviews_bigrams.gensim'.format(target_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our model, we can identify bi-grams in biographies. Now, we will construct a vocabulary dictionary: \n",
    "\n",
    "```\n",
    "{gender => {word => # of biographies}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary = defaultdict(Counter)\n",
    "\n",
    "for i, entity in enumerate(iter_entities_from(abstracts_file)):\n",
    "    resource = entity['resource']\n",
    "    if resource in person_data.index:\n",
    "        try:\n",
    "            abstract = entity['abstract'].pop()\n",
    "            if not abstract:\n",
    "                #some biographies have an empty abstract.\n",
    "                continue\n",
    "            \n",
    "            gender = person_data.loc[resource].gender\n",
    "            n_grams = bigrams[list(gensim.utils.tokenize(abstract, deacc=True, lowercase=True))]\n",
    "            vocabulary[gender].update(set(n_grams))\n",
    "        except KeyError:\n",
    "            # some biographies do not have an abstract.\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we save it in a structure to be used in the following notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with gzip.open('{0}/vocabulary.json.gz'.format(target_folder), 'wb') as f:\n",
    "    json.dump(vocabulary, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
