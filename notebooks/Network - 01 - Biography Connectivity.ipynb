{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Network Analysis - Generating Biography Networks\n",
    "\n",
    "In this notebook we generate a network of biographies from the link structure in the English DBpedia. On this network we will compare different centrality measures as well as the distribution of links between genders.\n",
    "\n",
    "By [Eduardo Graells-Garrido](http://carnby.github.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "from dbpedia_utils import iter_entities_from\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import gzip\n",
    "import dbpedia_config\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_folder = dbpedia_config.DATA_FOLDER\n",
    "target_folder = dbpedia_config.TARGET_FOLDER\n",
    "main_language = dbpedia_config.MAIN_LANGUAGE\n",
    "links_file = '{0}/page_links_{1}.nt.bz2'.format(source_folder, main_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load person data to process only biographies present in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gender_data = pd.read_csv('{0}/person_data_{1}.csv.gz'.format(target_folder, main_language), encoding='utf-8', index_col='uri').loc[:,('gender', 'birth_year')].reset_index().copy()\n",
    "gender_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point of time you can decide whether you want to process the entire biography network, or, for instance, consider only biographies born before/after 1900, and so on. You decide :)\n",
    "\n",
    "In this notebook, we will test the biographies of people born before 1900."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gender_data = gender_data[(gender_data.birth_year < 1900) & (gender_data.birth_year) > 0].reset_index(drop=True)\n",
    "gender_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that instead of indexing by URI as we do on the other notebooks, this time we indexed by a number because we use numbers to identify vertices in the graph.\n",
    "\n",
    "So we build a map from URIs to integer indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gender_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uri_to_i = {row.uri: idx for idx, row in gender_data.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_to_gender = {idx: row.gender[0] for idx, row in gender_data.iterrows()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [graphtool](http://graph-tool.skewed.de/) library to create our graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graph_tool\n",
    "\n",
    "def build_graph(dataframe, uri_to_i):\n",
    "    graph_data = {'graph': graph_tool.Graph(), 'id_map': {}, 'g_map': {}, 'pk_to_i': {}}\n",
    "    graph = graph_data['graph']\n",
    "    print('dataframe shape', dataframe.shape)\n",
    "    graph.add_vertex(dataframe.shape[0])\n",
    "\n",
    "    for entity in iter_entities_from(links_file):\n",
    "        resource = entity['resource']\n",
    "\n",
    "        if resource in uri_to_i:\n",
    "            #print(resource)\n",
    "            links = []\n",
    "            for other in entity['wikiPageWikiLink']:\n",
    "                if other in uri_to_i:\n",
    "                    #print('->', other)\n",
    "                    links.append(other)\n",
    "            src = uri_to_i[resource]\n",
    "            for link in links:\n",
    "                #print(src, link)\n",
    "                graph.add_edge(src, uri_to_i[link])\n",
    "            \n",
    "    graph_data['gender'] = graph.new_vertex_property('string')\n",
    "    gender = graph_data['gender']\n",
    "    \n",
    "    print(graph.num_vertices(), graph.num_edges())\n",
    "    \n",
    "    for i, row in dataframe.iterrows():\n",
    "        gender[i] = row.gender[0]\n",
    "        \n",
    "    graph.vertex_properties['gender'] = gender\n",
    "    \n",
    "    return graph_data\n",
    "\n",
    "graph_data = build_graph(gender_data, uri_to_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Not all nodes have links. In our analysis, we only considered nodes connected to at least one other node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cytoolz import frequencies\n",
    "from __future__ import division\n",
    "\n",
    "def count_values(graph, property_name):\n",
    "    values = []\n",
    "    for v in graph.vertices():\n",
    "        values.append(graph.vertex_properties[property_name][v])\n",
    "    return frequencies(values)\n",
    "\n",
    "def clean_isolated_nodes(graph_data):\n",
    "    graph = graph_data['graph']\n",
    "    degree_values = graph.degree_property_map('total').get_array()\n",
    "    degree_flag = degree_values.astype(np.bool)\n",
    "    \n",
    "    flags = graph.new_vertex_property('bool', vals=degree_flag)\n",
    "    \n",
    "    graph.set_vertex_filter(flags)\n",
    "    \n",
    "    print('to keep', np.sum(flags.get_array()), 'from', graph.num_vertices())\n",
    "    graph_data['kept_frequencies'] = count_values(graph, 'gender')\n",
    "    print(graph_data['kept_frequencies'])\n",
    "          \n",
    "    graph.clear_filters()\n",
    "    graph.set_vertex_filter(flags, inverted=True)\n",
    "          \n",
    "    print('to delete', np.sum(flags.get_array()), 'from', graph.num_vertices())\n",
    "    graph_data['deleted_frequencies'] = count_values(graph, 'gender')\n",
    "    print(graph_data['deleted_frequencies'])\n",
    "          \n",
    "    graph.clear_filters()\n",
    "    graph.set_vertex_filter(flags)\n",
    "    #graph.purge_vertices(in_place=True)\n",
    "    #graph.clear_filters()\n",
    "    \n",
    "clean_isolated_nodes(graph_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate how one gender is connected to itself and other genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "results = {}\n",
    "\n",
    "def gender_stats(graph_data):\n",
    "    graph = graph_data['graph']\n",
    "        \n",
    "    ratios = defaultdict(list)\n",
    "    edge_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for src in graph.vertices():\n",
    "        counts = Counter()\n",
    "\n",
    "        for dst in src.out_neighbours():\n",
    "            counts[graph.vertex_properties['gender'][dst]] += 1\n",
    "\n",
    "        if not counts:\n",
    "            continue\n",
    "\n",
    "        key = graph.vertex_properties['gender'][src]\n",
    "\n",
    "        if counts['m'] > 0:\n",
    "            ratio = counts['f'] / float(counts['m'])\n",
    "        else:\n",
    "            ratio = 0.0\n",
    "\n",
    "        ratios[key].append(ratio)\n",
    "        edge_counts[key]['m'] += counts['m']\n",
    "        edge_counts[key]['f'] += counts['f']\n",
    "\n",
    "        #print src_data['label'], counts\n",
    "    return ratios, edge_counts\n",
    "\n",
    "def build_ratios(graph_data, results):\n",
    "    graph = graph_data['graph']\n",
    "    \n",
    "    genders = ['m', 'f']\n",
    "    print('edge fractions')\n",
    "    \n",
    "    expected_counts = graph_data['kept_frequencies']\n",
    "    expected_freqs = [expected_counts['m'] / graph.num_vertices() * 100.0, expected_counts['f'] / graph.num_vertices() * 100.0, ]\n",
    "    print('expected freqs', expected_freqs)\n",
    "\n",
    "    ratios, edge_counts = gender_stats(graph_data)\n",
    "    results['gender_ratios'] = dict(ratios)\n",
    "    results['gender_edge_counts'] = dict(edge_counts)\n",
    "    #print('ratios', ratios)\n",
    "    print('edge counts', edge_counts)\n",
    "    \n",
    "build_ratios(graph_data, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can estimate centrality measures. In particular, we consider PageRank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graph_tool.centrality\n",
    "\n",
    "def centrality(graph_data, results):\n",
    "    graph = graph_data['graph']\n",
    "    \n",
    "    def estimate_fraction(method, name, fraction_name):\n",
    "        values = method(graph)\n",
    "        graph.vertex_properties[name] = values\n",
    "    \n",
    "        sorted_pr = []\n",
    "        for v in graph.vertices():\n",
    "            sorted_pr.append((graph.vertex_properties[name][v], v))\n",
    "        \n",
    "        print(name, graph.num_vertices(), len(sorted_pr))\n",
    "    \n",
    "        sorted_pr = sorted(sorted_pr, reverse=True)\n",
    "        #print(sorted_pr)\n",
    "        f_fraction = []\n",
    "        print(sorted_pr[:3])\n",
    "\n",
    "        count_w = 0.0\n",
    "\n",
    "        for i, (node_pr, node_id) in enumerate(sorted_pr, start=1):\n",
    "            #for i, (node_id, node_data) in enumerate(graph.nodes_iter(data=True), start=1):\n",
    "        \n",
    "            if graph.vertex_properties['gender'][node_id] == 'f':\n",
    "                count_w += 1.0\n",
    "        \n",
    "            f_fraction.append(count_w / i)\n",
    "        \n",
    "        results[fraction_name] = f_fraction\n",
    "        results['count_w'] = count_w\n",
    "            \n",
    "    estimate_fraction(graph_tool.centrality.pagerank, 'pagerank', 'f_fraction')\n",
    "    estimate_fraction(lambda x: x.degree_property_map('in'), 'in_degree', 'f_id_fraction')\n",
    "    return\n",
    "    \n",
    "centrality(graph_data, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the PageRank distribution with respect to how many women are present at each subset of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_fraction(graph_data, results, label, fraction='f_fraction'):\n",
    "    graph = graph_data['graph']\n",
    "    f_fraction = results[fraction]\n",
    "    count_w = results['count_w']\n",
    "    \n",
    "    #plt.figure(figsize=(6,4))\n",
    "    plt.plot(np.arange(1, len(f_fraction) + 1, 1), f_fraction, '-', alpha=0.75, label=label)\n",
    "    #plt.plot(np.arange(1, len(f_fraction) + 1, 1), null_f_fraction, '-', alpha=0.75, label='Null Model')\n",
    "    plt.hlines([count_w / graph.num_vertices()], 1.0, graph.num_vertices(), linestyle='dashed', alpha=0.5, linewidth=0.5)\n",
    "    #plt.savefig('./results/connectivity_observedpagerank_proportion.png', dpi=100, bbox_inches='tight')\n",
    "    \n",
    "plot_fraction(graph_data, results, 'Pre-1900')\n",
    "plt.xlabel('# of Biographies in the top-k results')\n",
    "plt.ylabel('Fraction of Women')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also display the top biographies to find the most central persons according to gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_entities(dataframe, graph_data, results, name='pagerank', n=30):\n",
    "    graph = graph_data['graph']\n",
    "    genders = ['m', 'f']\n",
    "    pagerank = graph.vertex_properties[name]\n",
    "    gender = graph.vertex_properties['gender']\n",
    "    \n",
    "    sorted_pr = []\n",
    "    for v in graph.vertices():\n",
    "        sorted_pr.append((pagerank[v], v, gender[v]))\n",
    "\n",
    "    print(name, graph.num_vertices(), len(sorted_pr))\n",
    "\n",
    "    sorted_pr = sorted(sorted_pr, reverse=True)\n",
    "    \n",
    "    top_pagerank = {}\n",
    "    \n",
    "    for g in genders:\n",
    "        values = list(filter(lambda x: x[2] == g, sorted_pr))[0:n]\n",
    "        top_pagerank[g] = [(dataframe.loc[x[1]].uri, x[0]) for x in values]\n",
    "    \n",
    "    results['top_pagerank'] = top_pagerank\n",
    "    return top_pagerank\n",
    "    \n",
    "def plot_top_30(top_pagerank):\n",
    "    people = []\n",
    "\n",
    "    for m, w in zip(top_pagerank['m'], top_pagerank['f']):\n",
    "        people.append({'woman': w[0], 'man': m[0], 'w_pr': w[1], 'm_pr': m[1]})\n",
    "\n",
    "    df = pd.DataFrame.from_records(people, index=range(1, 31))\n",
    "\n",
    "    plt.figure(figsize=(9,12))\n",
    "    plt.title('Top-30 Biographies per Gender')\n",
    "\n",
    "    plt.plot(df.m_pr, -df.index + 31, 'o', label=\"Men\")\n",
    "    plt.plot(df.w_pr, -df.index + 31, '^', label=\"Women\")\n",
    "    plt.xlabel('PageRank')\n",
    "    plt.yticks(np.arange(1, 31))\n",
    "    #plt.yticks(df.index, [n[1]['name'] for n in graph.nodes_iter(data=True)])#, fontsize='xx-small', rotation=45)\n",
    "    plt.ylim([0.5, 30.5])\n",
    "    #plt.xlim([-0.1,1.1])\n",
    "\n",
    "\n",
    "    #for i, (x0, x1) in enumerate(zip(bc_real, bc_base)):\n",
    "    #    plt.arrow(x0, i, x1 - x0, 0, length_includes_head=True)\n",
    "\n",
    "    plt.hlines(-df.index + 31, df.m_pr, df.w_pr, alpha=0.3, linestyle='dashed', linewidth=1)\n",
    "\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    ax1 = plt.gca()\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.grid(False)\n",
    "    ax2.set_ylim(ax1.get_ylim())\n",
    "    ax2.set_yticks(np.arange(1, 31))\n",
    "    ax2.set_yticklabels(list(df.man)[::-1])\n",
    "    ax1.set_yticklabels(list(df.woman)[::-1])\n",
    "    \n",
    "top_entities(gender_data, graph_data, results)\n",
    "plot_top_30(results['top_pagerank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Estimations\n",
    "\n",
    "We want to compare our network with baseline graphs that are, by construction, not biased. For more information you can check [our paper](http://arxiv.org/abs/1502.02341) on why we do this. Particularly we focus on:\n",
    "\n",
    "  * Small Worlds.\n",
    "  * Random Graphs.\n",
    "  * Random Graphs maintaining Full Degree Sequence.\n",
    "\n",
    "\n",
    "To do so we need to estimate some stats first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graph_tool.clustering\n",
    "\n",
    "def append_graph_stats(graph_data, results):\n",
    "    graph = graph_data['graph']\n",
    "    \n",
    "    results['n_nodes'] = graph.num_vertices()\n",
    "    results['n_edges'] = graph.num_edges()\n",
    "\n",
    "    print('directed', results['n_nodes'], results['n_edges'])\n",
    "\n",
    "    results['u_n_nodes'] = graph.num_vertices()\n",
    "    results['u_n_edges'] = graph.num_edges()\n",
    "    \n",
    "    k = (results['u_n_edges'] - (results['u_n_edges'] % 2)) / results['u_n_nodes']\n",
    "    results['k'] = k\n",
    "    print('k', k, results['u_n_nodes'], results['u_n_edges'])\n",
    "\n",
    "    clust_prop = graph_tool.clustering.local_clustering(graph)\n",
    "    clust_coeff = clust_prop.get_array().mean().tolist()\n",
    "    results['clust_coeff'] = clust_coeff\n",
    "    print('coeff', results['clust_coeff'])\n",
    "    \n",
    "append_graph_stats(graph_data, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to estimate the $\\beta$ parameter for the Small World graph we are going to generate. Note that we need to use [`networkx`](http://networkx.github.io/documentation/latest/reference/generated/networkx.generators.random_graphs.watts_strogatz_graph.html) because `graph_tool` does not have a Small World generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import brenth\n",
    "import networkx as nx\n",
    "\n",
    "def estimate_beta(graph_data, results):\n",
    "    k = int(np.round(results['k']))\n",
    "    print('k', k)\n",
    "    \n",
    "    def find_clustering(x):\n",
    "        g_nx = nx.watts_strogatz_graph(results['n_nodes'], k, x)    \n",
    "        nx.write_graphml(g_nx, '{0}/temp_graph.graphml'.format(target_folder))\n",
    "        g = graph_tool.Graph()\n",
    "        g.load('{0}/temp_graph.graphml'.format(target_folder))\n",
    "        g_prop = graph_tool.clustering.local_clustering(g)\n",
    "        g_coeff = g_prop.get_array().mean().tolist()\n",
    "        print(g_coeff - results['clust_coeff'])\n",
    "        return g_coeff - results['clust_coeff']\n",
    "    \n",
    "    beta = brenth(lambda x: (nx.cluster.average_clustering(nx.watts_strogatz_graph(results['n_nodes'], k, x)) - results['clust_coeff']), 0.0, 1.0) \n",
    "    print('beta', beta)\n",
    "    results['beta'] = beta\n",
    "\n",
    "estimate_beta(graph_data, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import random\n",
    "\n",
    "def prepare_baseline(graph, base_data):\n",
    "    graph_data = {'graph': graph, 'id_map': base_data['id_map'], \n",
    "                  'kept_frequencies': base_data['kept_frequencies']}\n",
    "    results = {}\n",
    "    build_ratios(graph_data, results)\n",
    "    centrality(graph_data, results)\n",
    "    append_graph_stats(graph_data, results)\n",
    "    return graph_data, results\n",
    "\n",
    "def generate_small_world(source_graph_data, source_results):\n",
    "    sw_graph = nx.watts_strogatz_graph(source_results['n_nodes'], int(np.round(source_results['k'])), \n",
    "                                       source_results['beta'], seed=31072010)\n",
    "\n",
    "    handle, filename = tempfile.mkstemp(suffix='.graphml')\n",
    "    nx.write_graphml(sw_graph, filename)\n",
    "    g = graph_tool.Graph()\n",
    "    g.load(filename)\n",
    "    \n",
    "    print(g.num_vertices(), g.num_edges())\n",
    "\n",
    "    freqs = source_graph_data['kept_frequencies']\n",
    "    shuffled = ['m'] * freqs['m'] + ['f'] * freqs['f']\n",
    "    random.shuffle(shuffled)\n",
    "    \n",
    "    g.vertex_properties['gender'] = g.new_vertex_property('string')\n",
    "    for i, gender in enumerate(shuffled, start=0):\n",
    "        g.vertex_properties['gender'][i] = gender\n",
    "    \n",
    "    sw_data, sw_results = prepare_baseline(g, source_graph_data)\n",
    "    #save_results(sw_data, sw_results, 'sw', 'Small World')\n",
    "    \n",
    "    return sw_data, sw_results\n",
    "    \n",
    "sw, sw_results = generate_small_world(graph_data, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `graph_tool` random module to perturb our network and build the baseline random networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graph_tool.generation\n",
    "\n",
    "def random_graph(source_graph_data, source_results, model='uncorrelated', n_iter=1):\n",
    "    clone = source_graph_data['graph'].copy()\n",
    "    graph_tool.generation.random_rewire(clone, model=model, n_iter=n_iter)\n",
    "    base_data, base_results = prepare_baseline(clone, source_graph_data)    \n",
    "    return base_data, base_results\n",
    "\n",
    "def populate_rand(rand, rand_results, model='erdos', n_iter=1):\n",
    "    random_g_data, random_g_results = random_graph(pre, results['pre'], model=model, n_iter=n_iter)\n",
    "    rand['pre'] = random_g_data\n",
    "    rand_results.update(random_g_results)\n",
    "\n",
    "rand, rand_results = random_graph(graph_data, results, model='erdos')\n",
    "deg_seq, deg_seq_results = random_graph(graph_data, results, model='uncorrelated', n_iter=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can plot the distributions of PageRank for each network. In this way, we can see how biased is the estimated centrality in comparison to unbiased networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_distributions(loc='best'):\n",
    "    plot_fraction(graph_data, results, 'PageRank (OBS)')\n",
    "    plot_fraction(graph_data, results, 'In-Degree (OBS)', fraction='f_id_fraction')\n",
    "    plot_fraction(sw, sw_results, 'Small World')\n",
    "    plot_fraction(rand, rand_results, 'Random')\n",
    "    plot_fraction(deg_seq, deg_seq_results, 'Degree Sequence')\n",
    "    plt.legend(loc=loc, fontsize='x-small')\n",
    "\n",
    "sns.set_palette('Set2')\n",
    "plot_distributions()\n",
    "plt.xlim([10, 1000000])\n",
    "plt.ylim([0, 0.3])\n",
    "plt.xscale('log')\n",
    "#plt.xlabel('Sorted Top-k Biographies')\n",
    "plt.ylabel('Fraction of Women')\n",
    "plt.title('0 -- 1900')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
