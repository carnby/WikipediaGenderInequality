{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Person Data\n",
    "\n",
    "In this notebook we generate a csv file with person data for all entities in the considered language editions. This includes asking Wikidata for the gender of unknown entities.\n",
    "\n",
    "We use the resulting dataset through all the other notebooks in this project.\n",
    "\n",
    "By [Eduardo Graells-Garrido](http://carnby.github.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import csv\n",
    "import regex as re\n",
    "import json\n",
    "import time\n",
    "import datetime \n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import dbpedia_config\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from cytoolz import partition_all\n",
    "from dbpedia_utils import iter_entities_from, get_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder = dbpedia_config.DATA_FOLDER\n",
    "target_folder = dbpedia_config.TARGET_FOLDER\n",
    "your_email = dbpedia_config.YOUR_EMAIL\n",
    "query_wikidata = dbpedia_config.QUERY_WIKIDATA_GENDER\n",
    "languages = dbpedia_config.LANGUAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to know the children classes of Person in the DBpedia ontology. We use rdflib and networkx to find them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import bz2file\n",
    "import networkx as nx\n",
    "import rdflib.graph as rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('{0}/dbpedia.owl'.format(data_folder), 'r') as f:\n",
    "    ontology = rdf.Graph().parse(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ontology_graph = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for s, p, o in ontology:\n",
    "    src = str(s)\n",
    "    attr = str(p)\n",
    "    dst = str(o)\n",
    "    #print(s, p, o)\n",
    "\n",
    "    if attr == 'http://www.w3.org/2000/01/rdf-schema#subClassOf':\n",
    "        ontology_graph.add_edge(dst, src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ontology_graph.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "person_classes = set(nx.neighbors(ontology_graph, 'http://dbpedia.org/ontology/Person'))\n",
    "len(person_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "person_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a variety of classes in the ontology, but we do not need as much detail. We build a dictionary to obtain the first child of the _Person_ class that matches each entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_parents = {}\n",
    "for level_1 in person_classes:\n",
    "    for descendant in nx.descendants(ontology_graph, level_1):\n",
    "        class_parents[descendant] = level_1\n",
    "    class_parents[level_1] = level_1\n",
    "    \n",
    "# to avoid querying another dictionary/set\n",
    "class_parents['http://dbpedia.org/ontology/Person'] = 'http://dbpedia.org/ontology/Person'\n",
    "len(class_parents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Wikidata, these are the identifiers for the different gender values.\n",
    "This is also the API URL and the headers we send when querying for entity genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "value_dict = {6581097: 'male', 6581072: 'female', 1052281: 'transgender female', 2449503: 'transgender male'}\n",
    "\n",
    "wikidata_api_url = 'http://www.wikidata.org/w/api.php?action=wbgetentities&ids={0}&format=json&props=claims'\n",
    "headers = {'user-agent': 'gender-research-crawler/0.0.1 (contact: {0})'.format(your_email)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we have two extra sources of gender information. Here we use them to query for gender of specific biographies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gender_by_dbpedia_uri = defaultdict(lambda: None)\n",
    "    \n",
    "for ent in iter_entities_from('{0}/wiki.genders.txt'.format(data_folder)):\n",
    "    gender_by_dbpedia_uri[ent['resource']] = ent['gender'].pop()\n",
    "        \n",
    "len(gender_by_dbpedia_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ent in iter_entities_from('{0}/genders_en.nt.bz2'.format(data_folder)):\n",
    "    if ent['resource'] not in gender_by_dbpedia_uri:\n",
    "        gender_by_dbpedia_uri[ent['resource']] = ent['gender'].pop()\n",
    "        \n",
    "len(gender_by_dbpedia_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this notebook has been run multiple times, we cache the Wikidata genders in order to avoid querying the system too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wikidata_gender = defaultdict(lambda: None)\n",
    "\n",
    "if os.path.exists('{0}/wikidata_entity_gender.json'.format(target_folder)):\n",
    "    with open('{0}/wikidata_entity_gender.json'.format(target_folder), 'r') as f:\n",
    "        wikidata_gender.update(json.load(f))\n",
    "    \n",
    "len(wikidata_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to avoid multiple queries of the same entity\n",
    "no_gender_available = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_entity_gender(req_json, entity_id):\n",
    "    \"\"\"\n",
    "    Given a JSON structure from Wikidata, get the gender of the specified entity.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ent_value = req_json['entities'][entity_id]['claims']['P21'][0]['mainsnak']['datavalue']['value']['numeric-id']\n",
    "        return value_dict[ent_value]\n",
    "    except KeyError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DBpedia data includes the relationships between editions of each article. This function decides whether such links are from a different language edition, or Wikidata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbpedia_url = re.compile(r'http://(.+)\\.dbpedia.org/*+')\n",
    "wikidata_url = re.compile(r'http://www.wikidata.org/entity/(.+)|http://wikidata.org/entity/(.+)')\n",
    "\n",
    "discarded_editions = {'simple', 'commons', 'wikidata'}\n",
    "\n",
    "def get_edition(url):\n",
    "    edition = None\n",
    "    wikidata_id = None\n",
    "    \n",
    "    if url.startswith('http://dbpedia.org/'):\n",
    "        edition = 'en'\n",
    "    else:\n",
    "        dbp_prefix = dbpedia_url.match(url)\n",
    "        if dbp_prefix:\n",
    "            prefix = dbp_prefix.groups()[0]\n",
    "\n",
    "            if prefix not in discarded_editions:\n",
    "                edition = prefix\n",
    "\n",
    "        else:\n",
    "            wikidata = wikidata_url.match(url)\n",
    "            if wikidata:\n",
    "                if wikidata.groups()[0]:\n",
    "                    wikidata_id = wikidata.groups()[0]\n",
    "                else:\n",
    "                    wikidata_id = wikidata.groups()[1]\n",
    "\n",
    "    return edition, wikidata_id\n",
    "\n",
    "get_edition('http://mg.dbpedia.org/resource/Paul_Otlet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function that creates a CSV file containing biographical meta-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_person_data(language, query_wikidata=True, skip_labels=False, skip_countries=False, include_without_gender=False):\n",
    "    \"\"\"\n",
    "    Creates a csv file with person data from the specified language edition. \n",
    "    If query_wikidata is true, entities not found in our gender dictionaries will be queried in Wikidata.\n",
    "    \"\"\"\n",
    "    # indexed by URI\n",
    "    person_uris = {}\n",
    "    person_ontologies = {}\n",
    "    person_birth = defaultdict(lambda: None)\n",
    "    person_birth_place = defaultdict(lambda: None)\n",
    "    person_death = defaultdict(lambda: None)\n",
    "    person_death_place = defaultdict(lambda: None)\n",
    "    person_gender = defaultdict(lambda: None)\n",
    "    person_editions = defaultdict(lambda: list([language]))\n",
    "    person_labels = defaultdict(lambda: None)\n",
    "    person_alternate_uri = defaultdict(lambda: None)\n",
    "    \n",
    "    country_dict = defaultdict(lambda: None)\n",
    "    countries = set()\n",
    "\n",
    "    if not skip_countries:\n",
    "        try:\n",
    "            print('{0}/countries_{1}.json.gz'.format(target_folder, language))\n",
    "            with gzip.open('{0}/countries_{1}.json.gz'.format(target_folder, language), 'rt') as f:\n",
    "                country_dict.update(json.load(f))\n",
    "                countries.update(country_dict.values())\n",
    "            print('# toponyms', len(country_dict))\n",
    "        except Exception as err:\n",
    "            print('error loading countries', err)\n",
    "            skip_countries = True\n",
    "\n",
    "        def get_country(toponyms):\n",
    "            for t in toponyms:\n",
    "                if t in countries:\n",
    "                    return t\n",
    "                if t in country_dict:\n",
    "                    return country_dict[t]\n",
    "            return None\n",
    "    \n",
    "    instance_types = '{1}/instance_types_{0}.ttl.bz2'.format(language, data_folder)\n",
    "    interlanguage_links = '{1}/interlanguage_links_{0}.ttl.bz2'.format(language, data_folder)\n",
    "    labels = '{1}/labels_{0}.ttl.bz2'.format(language, data_folder)\n",
    "    object_properties = '{1}/mappingbased_objects_{0}.ttl.bz2'.format(language, data_folder)\n",
    "    literal_properties = '{1}/mappingbased_literals_{0}.ttl.bz2'.format(language, data_folder)\n",
    "    \n",
    "    for i, ent in enumerate(iter_entities_from(instance_types)):\n",
    "        ent_class = ent['22-rdf-syntax-ns#type'].pop()\n",
    "        \n",
    "        if ent_class in class_parents:\n",
    "            person_uris[ent['resource']] = class_parents[ent_class]\n",
    "            \n",
    "    print('# persons', len(person_uris))\n",
    "    \n",
    "    entity_wikidata = defaultdict(lambda: None)\n",
    "    entity_uri = defaultdict(lambda: None)\n",
    "    without_gender = []\n",
    "\n",
    "    for i, ent in enumerate(iter_entities_from(literal_properties)):\n",
    "        resource = ent['resource']\n",
    "\n",
    "        if resource not in person_uris:\n",
    "            continue     \n",
    "            \n",
    "        if 'birthDate' in ent:\n",
    "            birth_year = get_date(ent, 'birthDate')\n",
    "        \n",
    "            if birth_year is not None:\n",
    "                person_birth[resource] = birth_year.year\n",
    "                \n",
    "        if 'deathDate' in ent:\n",
    "            death_year = get_date(ent, 'deathDate')\n",
    "        \n",
    "            if death_year is not None:\n",
    "                person_death[resource] = death_year.year\n",
    "                \n",
    "    if not skip_countries:\n",
    "        for i, ent in enumerate(iter_entities_from(object_properties)):\n",
    "            resource = ent['resource']\n",
    "\n",
    "            if resource not in person_uris:\n",
    "                continue     \n",
    "\n",
    "            if 'birthPlace' in ent:\n",
    "                place = get_country(ent['birthPlace'])\n",
    "                if place is not None:\n",
    "                    person_birth_place[resource] = place\n",
    "\n",
    "            if 'deathPlace' in ent:\n",
    "                place = get_country(ent['deathPlace'])\n",
    "                if place is not None:\n",
    "                    person_death_place[resource] = place\n",
    "    \n",
    "    if not skip_labels:\n",
    "        for i, ent in enumerate(iter_entities_from(labels)):\n",
    "            resource = ent['resource']\n",
    "\n",
    "            if not resource in person_uris:\n",
    "                continue\n",
    "        \n",
    "            if ent['rdf-schema#label']:\n",
    "                person_labels[resource] = ent['rdf-schema#label'].pop()\n",
    "            \n",
    "    \n",
    "    for i, ent in enumerate(iter_entities_from(interlanguage_links)):\n",
    "        resource = ent['resource']\n",
    "\n",
    "        if resource not in person_uris:\n",
    "            continue\n",
    "\n",
    "        this_entity_editions = set()\n",
    "        this_entity_wikidata = None\n",
    "        alt_url = None\n",
    "        \n",
    "        for url in ent['owl#sameAs']:\n",
    "            edition, wikidata_id = get_edition(url)\n",
    "            \n",
    "            if edition is not None:\n",
    "                this_entity_editions.add(edition)\n",
    "                if edition == 'en':\n",
    "                    alt_url = url\n",
    "                    \n",
    "            elif wikidata_id != None:\n",
    "                this_entity_wikidata = wikidata_id\n",
    "            \n",
    "        if alt_url is None:\n",
    "            alt_url = ent['owl#sameAs'].pop()\n",
    "        \n",
    "        person_alternate_uri[resource] = alt_url                \n",
    "        person_editions[resource].extend(this_entity_editions)\n",
    "        \n",
    "        if this_entity_wikidata:\n",
    "            entity_wikidata[resource] = this_entity_wikidata\n",
    "            entity_uri[this_entity_wikidata] = resource\n",
    "\n",
    "    \n",
    "    for ent_uri, ent_id in entity_wikidata.items():\n",
    "        if ent_uri in person_gender:\n",
    "            continue\n",
    "                \n",
    "        # do we know the URI?\n",
    "        if ent_uri in gender_by_dbpedia_uri:\n",
    "            person_gender[ent_uri] = gender_by_dbpedia_uri[ent_uri]\n",
    "        \n",
    "        # perhaps using same as...\n",
    "        if person_alternate_uri[ent_uri] is not None:\n",
    "            alt_uri = person_alternate_uri[ent_uri]\n",
    "            if alt_uri in gender_by_dbpedia_uri:\n",
    "                person_gender[ent_uri] = gender_by_dbpedia_uri[alt_uri]\n",
    "                continue\n",
    "            \n",
    "        # have we seen it on wikidata?\n",
    "        if ent_id in wikidata_gender:\n",
    "            person_gender[ent_uri] = wikidata_gender[ent_id]\n",
    "        elif ent_id not in no_gender_available:\n",
    "            without_gender.append(ent_id)\n",
    "\n",
    "    print('without gender', len(without_gender))\n",
    "\n",
    "    if query_wikidata:\n",
    "        for ids in partition_all(50, without_gender):\n",
    "            try:\n",
    "                req = requests.get(wikidata_api_url.format(u'|'.join(ids)), headers=headers)\n",
    "                req_json = req.json()\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            for i, ent_id in enumerate(ids):\n",
    "                ent_gender = get_entity_gender(req_json, ent_id)\n",
    "                \n",
    "                if ent_gender is None:\n",
    "                    no_gender_available.add(ent_id)\n",
    "                else:\n",
    "                    person_gender[entity_uri[ent_id]] = ent_gender\n",
    "                    wikidata_gender[ent_id] = ent_gender\n",
    "            \n",
    "\n",
    "    stats = dict(Counter(person_gender.values()))\n",
    "    stats['total_biographies'] = len(person_uris)\n",
    "    stats['language'] = language\n",
    "    stats['wikidata_entities'] = len(entity_wikidata)\n",
    "    stats['with_gender'] = len(person_gender)\n",
    "\n",
    "    with open('{1}/person_stats_{0}.json'.format(language, target_folder), 'w') as f:\n",
    "        json.dump(stats, f)\n",
    "\n",
    "    print(stats)\n",
    "\n",
    "    with gzip.open('{1}/person_data_{0}.csv.gz'.format(language, target_folder), 'wt') as f:\n",
    "        fields = ['uri', 'wikidata_entity', 'class', 'gender', 'edition_count', 'available_english', 'available_editions',\n",
    "                  'birth_year', 'death_year', 'birth_place', 'death_place', 'same_as', 'label']\n",
    "        writer = csv.DictWriter(f, fieldnames=fields)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for resource in person_uris.keys():\n",
    "            ent_gender = person_gender[resource]\n",
    "            \n",
    "            if ent_gender is None and not include_without_gender:\n",
    "                continue\n",
    "\n",
    "            writer.writerow({\n",
    "                    'wikidata_entity': entity_wikidata[resource],\n",
    "                    'uri': resource,\n",
    "                    'label': person_labels[resource] if person_labels[resource] else None,\n",
    "                    'gender': ent_gender,\n",
    "                    'available_english': 'en' in person_editions[resource],\n",
    "                    'edition_count': len(person_editions[resource]),\n",
    "                    'available_editions': u'|'.join(person_editions[resource]),\n",
    "                    'birth_year': person_birth[resource],\n",
    "                    'death_year': person_death[resource],\n",
    "                    'birth_place': person_birth_place[resource],\n",
    "                    'death_place': person_death_place[resource],\n",
    "                    'class': person_uris[resource],\n",
    "                    'same_as': person_alternate_uri[resource] if person_alternate_uri[resource] else None\n",
    "                })\n",
    "            \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(wikidata_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "records = []\n",
    "for lang in languages:\n",
    "    print(lang)\n",
    "    records.append(generate_person_data(lang, query_wikidata=query_wikidata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save this to be able to reuse this data in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('{0}/wikidata_entity_gender.json'.format(target_folder), 'w') as f:\n",
    "    json.dump(dict(wikidata_gender), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
